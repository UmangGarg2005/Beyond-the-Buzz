# Beyond-the-Buzz
To start with the code I immported basic modules like `pandas`, `numpy`, `tensorflow`, `sklearn.preprocessing`.Then I read the given file __train.csv__ using `pd.read_csv()`. As at the end we need to check the accuracy of the training I split the given data into two subdata using `sample()` function. Then I specified what is the input and output ('*x*' for input and '*y*' for output we desire ,i.e. __VERDICT__). Also as the there is the wide range of numbers in data from 100 to 1000000, to minimize the cost (or loss) the input data is scaled down using `StandardScalar()` and `transform()` functions.
Then layers of our neural network are created using `tensorflow.keras.Sequential()` which arranges the layers in linear form.There are __9 units__ in first layer which is equal to the number of columns in **x_train** and **x_val** followed by **64** and **32 units** in first hidden layer and second hidden layer respectively.Last layer consist of 1 unit.In first and second hidden layer we used `activation='relu'` because it is found to be more efficient in this case.On the other hand,in case of output `sigmoid` function is used to convert the given value between __0 to 1__.For compiling ,`adam` (adaptive moment estimation) optimatization is used which is a further extension of stochastic gradient descent to update network weights during training. Also `'binary crossentropy'` is ued for the `loss` attribute and `metrics='accuracy'` for evaluating accuracy at each step.Finally batch size and epochs are taken 60 and 50 respectively to make the program more efficient.
